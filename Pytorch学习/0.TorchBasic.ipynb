{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9aafa03548bd7e98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 第零节\n",
    "我们先学习一些 Pytorch 的基本知识，方便后面进行运算。\n",
    "\n",
    "这些知识包括：\n",
    "\n",
    "1.**张量**（Tensor）：PyTorch 的核心数据结构，支持多维数组，并可以在 CPU 或 GPU 上进行加速计算。\n",
    "\n",
    "2.**自动求导**（Autograd）：PyTorch 提供了自动求导功能，可以轻松计算模型的梯度，便于进行反向传播和优化。\n",
    "\n",
    "3.**神经网络**（nn.Module）：PyTorch 提供了简单且强大的 API 来构建神经网络模型，可以方便地进行前向传播和模型定义。\n",
    "\n",
    "4.**优化器**（Optimizers）：使用优化器（如 Adam、SGD 等）来更新模型的参数，使得损失最小化。\n",
    "\n",
    "5.**设备**（Device）：可以将模型和张量移动到 GPU 上以加速计算。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7aed1dd948aaa7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 张量\n",
    "张量（Tensor）是 PyTorch 中的核心数据结构，用于存储和操作多维数组。它可以在 CPU 和 GPU 上同时运算，兼容性很强。\n",
    "\n",
    "张量有三个重要的属性：**维度**（dimension）、**形状**(shape)和**数据类型**(Dtype)\n",
    "\n",
    "1.维度：张量的维度是指多维数组索引需要的参数量。例如一维张量就是一维数组、二维张量是矩阵，三维张量是一些矩阵的数组等。\n",
    "\n",
    "2.形状：张量在每一个维度上的大小决定了这个张量的形状，例如一个 3 * 2 的矩阵，作为一个二维张量，它的形状我们称作(3,2)\n",
    "\n",
    "3.数据类型：张量中存储的数据类型以及大小，Pytorch 中有整数型（如torch.int8、torch.int32）、\n",
    "  浮点型（如torch.float32、torch.float64）和布尔型（torch.bool）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4afc26dac0213e4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面我们来看张量的创建："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f65d9bfb1f5ceec6"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c215eabe8a348fd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. 创建全 0 张量，使用 torch.zeros(<Shape>) 方法\n",
    "all_zero = torch.zeros(3,2)\n",
    "print(all_zero)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2e4ee9eb958bf96",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. 创建全 1 张量，使用 torch.ones(<Shape>) 方法\n",
    "all_one = torch.ones(2,3)\n",
    "print(all_one)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a26fe9ca376b499",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. 从数组，元组创建张量，使用 torch.tensor(<List> or <Tuple>)方法\n",
    "by_list = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"用数组创建的张量是:\\n{}\".format(by_list))\n",
    "\n",
    "by_tuple = torch.tensor(((2,2),(3,4)))\n",
    "print(\"用元组创建的张量是:\\n{}\".format(by_tuple))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b6535e9ca1693a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. 从 Numpy 数组来创建张量，使用 torch.from_numpy(<Numpy.Array>) 方法\n",
    "import numpy as np\n",
    "x = np.array([[[1,2,3],[2,2,3]],[[4,5,6],[7,8,9]]])\n",
    "by_numpy = torch.from_numpy(x)\n",
    "print(by_numpy)\n",
    "\n",
    "# 注意：这种方法创建的 numpy 数组 x 和 by_numpy 是共享内存的，也就是更改一个的值，另外一个的也会更改。\n",
    "by_numpy[0] = 100\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a534475ec917ad9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 5*(不常用). 创建一个指定形状不指定内容的张量，使用 torch.Tensor(<Shape>) 方法\n",
    "unknown = torch.Tensor(5,6)\n",
    "print(unknown)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "435ee46b8c2392b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. 创建标准正态分布的随机数的张量，使用 torch.randn(<Shape>) 方法\n",
    "normal = torch.randn(5,6)\n",
    "print(normal)\n",
    "\n",
    "# 还可以设定种子的值，使用 torch.random.manual_seed(<Integer>) 方法\n",
    "torch.random.manual_seed(123456)\n",
    "one2six_normal = torch.randn(3,4)\n",
    "print(\"使用种子为123456的矩阵为：\\n{}\".format(one2six_normal))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b142012f5e829c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 7. 创建 0-1 均匀分布随机数的张量，使用 torch.rand(<Shape>) 方法\n",
    "torch.random.seed() # 清空种子\n",
    "uniform = torch.rand((3,4))\n",
    "print(uniform)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0c7211e0fd140f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 8. 创建从 a-b 的随机整数的均匀分布张量，使用 torch.randint(<Min>,<Max>,<Shape>)\n",
    "one_to_ten_rand = torch.randint(1,10,(2,3))\n",
    "print(one_to_ten_rand)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b88949d0a49faaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "9. 指定创建时的数据类型，可以使用 torch.IntTensor(), torch.FloatTensor(), torch.DoubleTensor().\n",
    "创建类型分别为：torch.int32, torch.float32 和 torch.float64 的张量。\n",
    "但是更推荐用：torch.tensor(<Data>, dtype = <Type>) 来创建\n",
    "'''\n",
    "tensor_float_64 = torch.tensor([[1,2],[1,2]],dtype = torch.float64)\n",
    "print(tensor_float_64)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f7f0cf56e99839b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "''' \n",
    "10. 创建从 a-b 但不包含b的，间隔为 1 的线性1维张量，使用 torch.arange(a,b)\n",
    "创建从 a-b 但不包含b的，间隔为 c 的线性1维张量，使用 torch.arange(a,b,c)\n",
    "'''\n",
    "one_to_ten_one = torch.arange(1,10)\n",
    "print(\"1到10（不含10）的1维张量，使用torch.arange(1,10)，结果为:\\n {}\".format(one_to_ten_one))\n",
    "one_to_ten_three = torch.arange(1,10,3)\n",
    "print(\"1到10（不含10），间隔为3的1维张量，使用torch.arange(1,10,3)，结果为:\\n {}\".format(one_to_ten_three))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2c803ba5ed96cbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 11. 创建将 a-b（包含b）平均分成 c 份的 1 维张量，用 torch.linspace(a,b,c) 方法\n",
    "zero_to_one_five = torch.linspace(0,1,5)\n",
    "print(zero_to_one_five)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75ff5c422e4b906c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 12. 创建一个和输入张量一样大小的全 1 或全 0 张量，使用 torch.ones_like(<Tensor>) , torch.zeros_like(<Tensor>)\n",
    "tensor1 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "like_ones = torch.ones_like(tensor1)\n",
    "like_zeros = torch.zeros_like(tensor1)\n",
    "\n",
    "print(\"第一个张量是：\\n{},\\n和它一样大的全 1 张量是：\\n {},\\n和它一样大的全 0 张量是：\\n {}\".format(tensor1, like_ones, like_zeros))\n",
    "\n",
    "# 此外，使用 torch.full(<Size>, <Number>) 和 torch.full_like(<Tensor>) 还可以生成填充为数字 <Number> 的张量。\n",
    "all_five = torch.full((2,3),5)\n",
    "like_full = torch.full_like(tensor1,5)\n",
    "\n",
    "print(\"\\n全是 5 的张量是：\\n{},\\n和第一个张量大小一样，全是 5 的张量是：\\n{}\".format(all_five,like_full))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "158a4f07a92b067c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 13.指定在哪个设备上创建（CPU 或 GPU），使用 torch.device(<str:Device>)\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "d = torch.randn(2,3,device=dev)\n",
    "print(d)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dc56c17baa89239",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面我们来看张量在 Pytorch 中的基本计算"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0462a951a95ee5b"
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. 张量加法，使用运算符加号（+）: z = x + y\n",
    "x = torch.tensor([[1,2],[3,4]])\n",
    "y = torch.tensor([[-1,0],[2,-1]])\n",
    "print(\"x = \\n{},\\ny = \\n{},\\nx+y = {}\".format(x,y,x+y))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34215c86a620624d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. 相同形状的张量对应元素乘积，使用乘号（*）: z = x * y\n",
    "print(x * y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24de1dc950399283",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. 符合条件的张量进行矩阵乘法，使用 torch.matmul(x,y) 或 直接 x @ y\n",
    "x1 = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "y1 = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[0,1,2]])\n",
    "print(x1 @ y1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bcdca26685fcba4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. 张量 x 的转置，使用 x.t() 或者 x.transpose() 方法\n",
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(\"张量 x = \\n{},\\n张量 x 的转置 = \\n{}\".format(x, x.t()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7d6b3a6f65946a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 5.返回张量 x 的形状，使用 x.shape 属性\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "705b52a6cf90cdf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. 张量可以进行数据类型转换，使用的是 x.type(<DataType>)\n",
    "x = torch.tensor([[0,0],[1,1]])\n",
    "x.type(torch.float64)\n",
    "\n",
    "# 输出张量的类型，使用的是 x.dtype 属性\n",
    "print(x.dtype)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a325d1049d7468d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    " # 7. 张量可以转化为 numpy 数组，使用的是 x.numpy() 方法\n",
    "print(x.numpy())\n",
    "\n",
    "# 但是它们会共享内存，即改变其中一个，另外一个也会改变\n",
    "x = x + torch.tensor([[-1,-1],[2,2]])\n",
    "print(\"将 x 更改后，x = \\n{},\\n而 x 的 numpy 数组是：\\n{}\".format(x, x.numpy()))\n",
    "\n",
    "# 使用 x.numpy().copy() 方法，复制一份 x 中的内容即可解决此现象：\n",
    "x_copy = x.numpy().copy()\n",
    "x = x + torch.tensor([[-1,-1],[2,2]])\n",
    "print(\"将 x 更改后，x = \\n{},\\n而 x 的 numpy 数组的 copy 是：\\n{}\".format(x, x_copy))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ce1122f79f8a87b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 8. 当张量中（无论维度）只有一个元素时，可以使用 x.item() 提取出这个元素\n",
    "x1 = torch.tensor(1)\n",
    "x2 = torch.tensor([1])\n",
    "x3 = x1 = torch.tensor([[[[1]]]])\n",
    "\n",
    "print(x1.item())\n",
    "print(x2.item())\n",
    "print(x3.item())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d7efb118b41aa77",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 9. 张量的拼接，使用 torch.cat([<Tensor1>, <Tensor2>]) 的方法将两个张量按照给定的方式拼接在一起\n",
    "torch.manual_seed(0)\n",
    "x1 = torch.randint(1,10,[2,3,4])\n",
    "x2 = torch.randint(1,10,[2,3,4])\n",
    "\n",
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "\n",
    "# 按第 0 个维度拼接\n",
    "x3 = torch.cat([x1, x2], dim = 0)\n",
    "print(\"按第 0 个维度拼接的结果是：{}\".format(x3.shape))\n",
    "\n",
    "# 按第 1 个维度拼接\n",
    "x4 = torch.cat([x1, x2], dim = 1)\n",
    "print(\"按第 1 个维度拼接的结果是：{}\".format(x4.shape))\n",
    "\n",
    "# 按第 2 个维度拼接\n",
    "x5 = torch.cat([x1, x2], dim = 2)\n",
    "print(\"按第 2 个维度拼接的结果是：{}\".format(x5.shape))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ad7537fb61eb17",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 10. 当我们想要将两个张量叠加成一个张量时，使用的是 torch.stack() 方法\n",
    "torch.manual_seed(0)\n",
    "x1 = torch.randint(1,10,[2,3])\n",
    "x2 = torch.randint(1,10,[2,3])\n",
    "\n",
    "# 按照第 0 个维度叠加时，是从第一个矩阵取完所有的列，然后作为第一个元素\n",
    "x3 = torch.stack([x1,x2],dim = 0)\n",
    "print(\"按第 0 个维度叠加的结果是：{}\".format(x3.shape))\n",
    "print(x3)\n",
    "\n",
    "# 按照第 1 个维度叠加时，是从每一个矩阵取完所有的第一行，然后作为第一个元素\n",
    "x4 = torch.stack([x1,x2],dim = 1)\n",
    "print(\"按第 1 个维度叠加的结果是：{}\".format(x4.shape))\n",
    "print(x4)\n",
    "\n",
    "# 按照第 2 个维度叠加时，是从每一个矩阵取完所有的第一列，然后作为第一个元素\n",
    "x5 = torch.stack([x1,x2],dim = 2)\n",
    "print(\"按第 2 个维度叠加的结果是：{}\".format(x5.shape))\n",
    "print(x5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6cffe2d3f5b7eaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面我们将介绍张量的索引操作，以2维张量为例："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ceea437fd66847a"
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. 行索引. 使用 x[<index>] 获得指定的某行元素\n",
    "data = torch.randint(0, 10, [4,5])\n",
    "print(data)\n",
    "print(data[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59ee7976b78229b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. 列索引，使用 x[:,<index>] 获得指定的某列元素\n",
    "print(data)\n",
    "print(data[:,0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1b8395ddfc026b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. 某个位置的元素，使用 x[<index1>][<index2>]或 x[<index1>,<index2>]获得指定位置的元素\n",
    "print(data)\n",
    "print(data[0][0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "204507702aa7ce2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 4.获得 a 到 b 行，c 到 d 列的元素，使用 x[a:b+1, c:d+1] 来实现，如果是到最后一行（列），只写 <index>: 即可\n",
    "print(data)\n",
    "print(data[1:3, 2:4])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93c8b1ec2febd769",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. 不连续地取某些行，列的交叉值，使用列表查询： x[<List1>, <List2>]. 注意列表的长度要统一。\n",
    "print(data)\n",
    "print(data[[0,2,3],[0,1,4]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b38bb8d0cb1d4955",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 6.取某些行，某些列的全体，使用 x[[<list1>],<list2>] 或 x[<list1>,[<list2>]]\n",
    "print(data)\n",
    "print(data[[[0],[2],[3]],[0,1,4]])\n",
    "print(data[[0,2,3],[[0],[1],[4]]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52a5f0bcd0954bf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 7. 对某些条件进行索引使用布尔索引\n",
    "print(data)\n",
    "\n",
    "# 希望获得该张量中所有大于 3 的元素，使用张量 > 3 得到一个布尔张量\n",
    "print(data > 3)\n",
    "\n",
    "# 输出其中大于 3 的，输出为一个 1 维张量\n",
    "print(data[data > 3])\n",
    "\n",
    "# 如果希望返回所有第三列元素大于 4 的行，类似地方法：\n",
    "print(data[:, 2] > 4)\n",
    "print(data[data[:,2] > 4])\n",
    "\n",
    "# 返回所有第 2 行大于 3 的列\n",
    "print(data[2] > 3)\n",
    "print(data[:, data[2] > 3])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc92d8f379fef6fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2e02a5d1030b923e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b236b5efe6c9b424",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 8. 多维数组的索引，与上面类似\n",
    "torch.manual_seed(0)\n",
    "x = torch.randint(0, 10, [2,3,4])\n",
    "print(x)\n",
    "print(x[0, 1, 1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9db5a11c55ed5362",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面我们将学习和张量形状相关的操作"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8d98ba951a763ca"
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. 改变张量的形状，使用 x.reshape(<Shape>) 方法. 注意元素个数要相同\n",
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x.shape, x.shape[0], x.shape[1])\n",
    "print(x.size(), x.size(0), x.size(1))\n",
    "\n",
    "y = x.reshape((1,6))\n",
    "print(y)\n",
    "\n",
    "# 使用 -1 省略第二个形状变量\n",
    "print(x.reshape(6,-1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d829b74698411a64",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. 张量的维度交换，可以用 x.transpose() 和 x.permute()\n",
    "torch.manual_seed(0)\n",
    "x = torch.randint(0,10,[2,3,4])\n",
    "print(x.shape)\n",
    "# transcope 可以交换两个维度\n",
    "print(torch.transpose(x, 0, 1).shape)\n",
    "# permute 可以交换多个维度\n",
    "print(torch.permute(x, [1, 2, 0]))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f61a7cfaa12a085",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 3.此外，改变张量的形状也可以用 x.view() 和 x.contigous(). 但是只能用整块内存存储的数据. 例如使用过 transcope 和 permute 的张量就不能用\n",
    "torch.manual_seed(0)\n",
    "data = torch.randint(0,10,[4,5])\n",
    "new_data = data.view(2,-1)\n",
    "print(new_data)\n",
    "data = torch.transpose(data,0,1)\n",
    "print(data.is_contiguous())\n",
    "new_data = data.contiguous().view(2,-1)\n",
    "print(new_data.is_contiguous())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d5d025ea83fc088",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. 如果我们想要删除 Shape 中维度为 1 的维度，可以使用 x.squeeze() 方法\n",
    "torch.manual_seed(0)\n",
    "data = torch.randint(0,10,[4,5,1,1,1])\n",
    "print(data.shape)\n",
    "\n",
    "# 可以一个一个手动去除\n",
    "new_data = data.squeeze(2)\n",
    "print(new_data.shape)\n",
    "# 或者不加参数，默认全部去除\n",
    "new_data = data.squeeze()\n",
    "print(new_data.shape)\n",
    "# 反之可以使用 x.unsqueeze() 手动增加一个在指定位置的 1 维的维度\n",
    "large_data = new_data.unsqueeze(1)\n",
    "print(large_data.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bec500cc85b019fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Pytorch 的计算函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34c696e69fcc5f06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "这一节我们来学习 Pytorch 中的一些计算函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8377e01671840d4a"
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. 计算均值，使用 x.mean() 方法\n",
    "torch.manual_seed(0)\n",
    "data = torch.randint(0,10,[4,5],dtype=torch.float64)\n",
    "# 对所有数据进行一个均值\n",
    "print(data)\n",
    "print(data.mean())\n",
    "\n",
    "# 在指定维度计算均值\n",
    "print(data.mean(dim=0))\n",
    "print(data.mean(dim=1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be01af7fb4888453",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. 求和，使用 x.sum() 方法\n",
    "print(data)\n",
    "print(data.sum())\n",
    "print(data.sum(dim=0))\n",
    "print(data.sum(dim=1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aedd81cdb6205063",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. 所有元素都次方，使用 x.pow(n)\n",
    "print(data)\n",
    "print(data.pow(2))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d3e6874567e6a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 4.平方根，使用 x.sqrt()\n",
    "print(data)\n",
    "print(data.sqrt())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f695c106c3d63c96",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 5.取 e 的指数，使用 x.exp() 方法\n",
    "print(data)\n",
    "print(data.exp())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29f6af480b909992",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. 取一个自然对数，使用 x.log()，以 m 为底使用 x.log(m)\n",
    "print(data)\n",
    "print(data.log())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f11a201841957b43",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "162d7283f7944ddf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面我们来学习 Pytorch 中的自动微分模块\n",
    "\n",
    "Pytorch 的自动微分使用的是反向传播的算法，即 x.backward()\n",
    "\n",
    "下面是具体的计算步骤"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c267e7483b9bc6d"
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. 对于单标量的梯度的计算\n",
    "\n",
    "# 如果想要计算梯度，必须加上 requires_grad = True 参数，并尽量定义成 torch.float64 类型及提高精度\n",
    "x = torch.tensor(10, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "# 对 x 中间的计算\n",
    "f = x ** 2 + 20\n",
    "# 开始求导\n",
    "f.backward()\n",
    "# 访问梯度可以使用 x.grad 属性\n",
    "print(x.grad)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c74cd961ead18ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. 对向量进行梯度计算\n",
    "x = torch.tensor([10,20,30,40],requires_grad = True, dtype=torch.float64)\n",
    "\n",
    "y1 = x ** 2 + 20\n",
    "\n",
    "# 开始自动求导\n",
    "# y1.backward()\n",
    "\n",
    "'''\n",
    "结果报错，因为微分函数 x.backward() 只能输入标量\n",
    "--------------------------------------------------------\n",
    "RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "--------------------------------------------------------\n",
    "'''\n",
    "\n",
    "# 我们可以用 y1 的一些值来标量化，如均值和求和\n",
    "y2 = y1.mean() # 此时 y2 是 y1 中的数字除以了 4 \n",
    "y2.backward()\n",
    "\n",
    "# 输出 x.grad\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adfb1cdc57b8608",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. 多元函数的梯度的计算\n",
    "x1 = torch.tensor(10, requires_grad=True, dtype=torch.float64)\n",
    "x2 = torch.tensor(10, requires_grad=True, dtype=torch.float64)\n",
    "x3 = torch.tensor(10, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "y = x1 ** 2 + x2 ** 2 + x3 ** 2\n",
    "\n",
    "y.backward()\n",
    "print(x1.grad)\n",
    "print(x2.grad)\n",
    "print(x3.grad)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4a2983cd799e00e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. 多元向量值函数的计算\n",
    "x1 = torch.tensor([10, 20], requires_grad=True, dtype=torch.float64)\n",
    "x2 = torch.tensor([30, 20], requires_grad=True, dtype=torch.float64)\n",
    "x3 = torch.tensor([40, 20], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "y1 = x1 ** 2 + x2 ** 2 + x3 ** 2\n",
    "y2 = y1.sum()\n",
    "y2.backward()\n",
    "\n",
    "print(x1.grad)\n",
    "print(x2.grad)\n",
    "print(x3.grad)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "773f7ea7bbce4418",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. 下面我们学习如何不进行梯度计算\n",
    "x = torch.tensor(10, requires_grad=True, dtype=torch.float64)\n",
    "print(x.requires_grad)\n",
    "\n",
    "# 1 为了不进行梯度计算，可以使用 with torch.no_grad(): 环境\n",
    "with torch.no_grad():\n",
    "    y = x ** 2\n",
    "    print(y.requires_grad)\n",
    "\n",
    "# 2 或者使用一个函数，声明上 @torch.no_grad() 函数内部进行计算：\n",
    "@torch.no_grad()\n",
    "def my_func(x):\n",
    "    return x ** 2\n",
    "\n",
    "# 3 直接设定所有代码不进行梯度计算，使用 torch.set_grad_enabled(False)\n",
    "torch.set_grad_enabled(False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aee50dd15081d270",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 2.累计梯度和梯度清零\n",
    "# 梯度的计算是累积在自变量的 grad 中的\n",
    "x = torch.tensor([10,20,30,40], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "for _ in range(3):\n",
    "    f1 = x ** 2 + 20\n",
    "    f2 = f1.sum()\n",
    "    f2.backward()\n",
    "    print(x.grad)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1da2ad542fdd8c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "当我们重复对 x 进行梯度计算时，会将历史的梯度值累加到 x 的 x.grad 属性中"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aceb3ac4338c8f0e"
  },
  {
   "cell_type": "code",
   "source": [
    "# 不累加历史梯度的方法，使用梯度清零方法 x.grad.data.zero_()\n",
    "# 注意使用条件句，否则 x.grad 可能是 None 报错\n",
    "if x.grad is not None:\n",
    "    x.grad.data.zero_()\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "490273947820fcea",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面我们举一个例子"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4834e5c12849f885"
  },
  {
   "cell_type": "code",
   "source": [
    "# 我们来求解 当 x（参数） 为什么值时， y = x ** 2 时最小\n",
    "# 先随机初始化一个值\n",
    "x = torch.tensor(10, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "for _ in range(2000):\n",
    "    # 先计算\n",
    "    y = x ** 2\n",
    "    \n",
    "    # 梯度清零\n",
    "    if x.grad is not None:\n",
    "        x.grad.data.zero_()\n",
    "    \n",
    "    # 自动微分\n",
    "    y.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    x.data = x.data - 0.01 * x.grad\n",
    "    \n",
    "    # 打印一下 x 的值\n",
    "print('%.10f' % x.data)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62daf099a5d37d41",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们来看一些需要注意的事项"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75c75e1cecdf58d6"
  },
  {
   "cell_type": "code",
   "source": [
    "# 当张量设置的 requires_grad=True 时，如果将该张量转化为 Numpy 数组时会报错\n",
    "x = torch.tensor([10,20,30], requires_grad=True,dtype=torch.float64)\n",
    "y = x.numpy()\n",
    "'''\n",
    "-------------------------------------------------------\n",
    "RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "-------------------------------------------------------\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7fe0f3b5f6851b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 因此我们要使用 x.detach() 创建一个没有梯度计算的张量\n",
    "x = torch.tensor([10,20,30], requires_grad=True,dtype=torch.float64)\n",
    "x1 = x.detach()\n",
    "y = x1.numpy()\n",
    "\n",
    "# 该新的张量与旧张量共享数据\n",
    "print(id(x.data), id(x1.data))\n",
    "x1[0] = 0\n",
    "print('当x1[0]=0时，x = \\n{},\\nx1 = \\n{}'.format(x,x1))\n",
    "\n",
    "# 注意对 x 进行改变时会对梯度进行计算，而 x1 的改变不会影响梯度的计算"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9a06efa319fdb1d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
